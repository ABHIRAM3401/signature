{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üõ°Ô∏è Siamese Network for Signature Verification\n",
                "\n",
                "**Training a signature verification model using Siamese Networks with MobileNetV2 backbone**\n",
                "\n",
                "- Dataset: CEDAR + BHSig260 (Hindi & Bengali)\n",
                "- Model: Siamese Network with shared weights\n",
                "- Loss: Binary Cross Entropy\n",
                "- GPU: Kaggle P100/T4"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1: Install Dependencies (if needed)\n",
                "!pip install torch torchvision tqdm matplotlib pillow scikit-learn --quiet\n",
                "print(\"‚úÖ Dependencies installed!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2: Imports and Configuration\n",
                "import os\n",
                "import random\n",
                "import time\n",
                "from itertools import combinations\n",
                "from PIL import Image\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torchvision import models, transforms\n",
                "from tqdm.notebook import tqdm\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Check GPU\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3: Configuration\n",
                "class Config:\n",
                "    # Dataset paths (Kaggle)\n",
                "    DATASET_BASE = '/kaggle/input/handwritten-signature-datasets'\n",
                "    CEDAR_PATH = os.path.join(DATASET_BASE, 'signatures/signatures')\n",
                "    # BHSig paths - adjust based on actual structure\n",
                "    BHSIG_HINDI = os.path.join(DATASET_BASE, 'BHSig260/Hindi')\n",
                "    BHSIG_BENGALI = os.path.join(DATASET_BASE, 'BHSig260/Bengali')\n",
                "    \n",
                "    # Image settings\n",
                "    IMAGE_SIZE = (155, 220)\n",
                "    GRAYSCALE = True\n",
                "    \n",
                "    # Model settings\n",
                "    EMBEDDING_DIM = 512\n",
                "    BACKBONE = 'mobilenet_v2'\n",
                "    \n",
                "    # Training settings\n",
                "    BATCH_SIZE = 64  # Increased for GPU\n",
                "    LEARNING_RATE = 0.0001\n",
                "    NUM_EPOCHS = 50\n",
                "    TRAIN_SPLIT = 0.8\n",
                "    NUM_WORKERS = 4  # Multi-process data loading\n",
                "    \n",
                "    # Checkpoints\n",
                "    CHECKPOINT_INTERVAL = 10\n",
                "    EARLY_STOPPING_PATIENCE = 10\n",
                "    SAVE_PATH = '/kaggle/working/checkpoints'\n",
                "    \n",
                "    RANDOM_SEED = 42\n",
                "\n",
                "config = Config()\n",
                "os.makedirs(config.SAVE_PATH, exist_ok=True)\n",
                "print(\"‚úÖ Configuration loaded!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4: Explore Dataset Structure\n",
                "print(\"üìÅ Dataset Structure:\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "# List top-level\n",
                "if os.path.exists(config.DATASET_BASE):\n",
                "    for item in os.listdir(config.DATASET_BASE):\n",
                "        path = os.path.join(config.DATASET_BASE, item)\n",
                "        if os.path.isdir(path):\n",
                "            count = len(os.listdir(path))\n",
                "            print(f\"üìÇ {item}/ ({count} items)\")\n",
                "            # Show first few items\n",
                "            for sub in os.listdir(path)[:3]:\n",
                "                print(f\"   ‚îî‚îÄ‚îÄ {sub}\")\n",
                "        else:\n",
                "            print(f\"üìÑ {item}\")\n",
                "else:\n",
                "    print(\"‚ùå Dataset not found! Make sure to add the dataset to your notebook.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5: Dataset Classes\n",
                "\n",
                "def get_file_type(filename, dataset_type):\n",
                "    \"\"\"Determine if file is genuine or forged\"\"\"\n",
                "    filename_lower = filename.lower()\n",
                "    \n",
                "    if dataset_type == 'cedar':\n",
                "        if 'original' in filename_lower:\n",
                "            return 'genuine'\n",
                "        elif 'forgeries' in filename_lower:\n",
                "            return 'forged'\n",
                "    else:  # bhsig\n",
                "        if '-g-' in filename_lower:\n",
                "            return 'genuine'\n",
                "        elif '-f-' in filename_lower:\n",
                "            return 'forged'\n",
                "    return None\n",
                "\n",
                "\n",
                "def load_dataset_images(dataset_path, dataset_type):\n",
                "    \"\"\"Load images organized by person\"\"\"\n",
                "    data = {}\n",
                "    \n",
                "    if not os.path.exists(dataset_path):\n",
                "        print(f\"Warning: {dataset_path} not found\")\n",
                "        return data\n",
                "    \n",
                "    for root, dirs, files in os.walk(dataset_path):\n",
                "        for file in files:\n",
                "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')):\n",
                "                file_path = os.path.join(root, file)\n",
                "                file_type = get_file_type(file, dataset_type)\n",
                "                \n",
                "                if file_type is None:\n",
                "                    continue\n",
                "                \n",
                "                # Extract person ID from path\n",
                "                rel_path = os.path.relpath(root, dataset_path)\n",
                "                if rel_path == '.':\n",
                "                    continue\n",
                "                person_id = f\"{dataset_type}_{rel_path.split(os.sep)[0]}\"\n",
                "                \n",
                "                if person_id not in data:\n",
                "                    data[person_id] = {'genuine': [], 'forged': []}\n",
                "                \n",
                "                data[person_id][file_type].append(file_path)\n",
                "    \n",
                "    return data\n",
                "\n",
                "\n",
                "def generate_balanced_pairs(data, max_pairs_per_person=50):\n",
                "    \"\"\"Generate balanced genuine and forgery pairs\"\"\"\n",
                "    genuine_pairs = []\n",
                "    forgery_pairs = []\n",
                "    \n",
                "    for person_id, images in data.items():\n",
                "        genuine_images = images['genuine']\n",
                "        forged_images = images['forged']\n",
                "        \n",
                "        if len(genuine_images) < 2:\n",
                "            continue\n",
                "        \n",
                "        # Genuine pairs\n",
                "        person_genuine = list(combinations(genuine_images, 2))\n",
                "        random.shuffle(person_genuine)\n",
                "        person_genuine = person_genuine[:max_pairs_per_person]\n",
                "        \n",
                "        # Forgery pairs\n",
                "        person_forgery = [(g, f) for g in genuine_images for f in forged_images]\n",
                "        random.shuffle(person_forgery)\n",
                "        person_forgery = person_forgery[:max_pairs_per_person]\n",
                "        \n",
                "        genuine_pairs.extend(person_genuine)\n",
                "        forgery_pairs.extend(person_forgery)\n",
                "    \n",
                "    # Balance\n",
                "    min_pairs = min(len(genuine_pairs), len(forgery_pairs))\n",
                "    random.shuffle(genuine_pairs)\n",
                "    random.shuffle(forgery_pairs)\n",
                "    genuine_pairs = genuine_pairs[:min_pairs]\n",
                "    forgery_pairs = forgery_pairs[:min_pairs]\n",
                "    \n",
                "    # Combine\n",
                "    pairs = genuine_pairs + forgery_pairs\n",
                "    labels = [1] * len(genuine_pairs) + [0] * len(forgery_pairs)\n",
                "    \n",
                "    combined = list(zip(pairs, labels))\n",
                "    random.shuffle(combined)\n",
                "    pairs, labels = zip(*combined)\n",
                "    \n",
                "    return list(pairs), list(labels)\n",
                "\n",
                "\n",
                "class SignatureDataset(Dataset):\n",
                "    def __init__(self, pairs, labels, transform=None):\n",
                "        self.pairs = pairs\n",
                "        self.labels = labels\n",
                "        self.transform = transform\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.pairs)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        img1_path, img2_path = self.pairs[idx]\n",
                "        label = self.labels[idx]\n",
                "        \n",
                "        img1 = Image.open(img1_path).convert('L' if config.GRAYSCALE else 'RGB')\n",
                "        img2 = Image.open(img2_path).convert('L' if config.GRAYSCALE else 'RGB')\n",
                "        \n",
                "        if self.transform:\n",
                "            img1 = self.transform(img1)\n",
                "            img2 = self.transform(img2)\n",
                "        \n",
                "        return img1, img2, torch.tensor(label, dtype=torch.float32)\n",
                "\n",
                "print(\"‚úÖ Dataset classes defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 6: Load and Prepare Data\n",
                "random.seed(config.RANDOM_SEED)\n",
                "\n",
                "print(\"üì¶ Loading datasets...\")\n",
                "\n",
                "all_data = {}\n",
                "\n",
                "# Load CEDAR\n",
                "if os.path.exists(config.CEDAR_PATH):\n",
                "    cedar_data = load_dataset_images(config.CEDAR_PATH, 'cedar')\n",
                "    all_data.update(cedar_data)\n",
                "    print(f\"  CEDAR: {len(cedar_data)} persons\")\n",
                "\n",
                "# Load BHSig Hindi\n",
                "if os.path.exists(config.BHSIG_HINDI):\n",
                "    hindi_data = load_dataset_images(config.BHSIG_HINDI, 'bhsig')\n",
                "    all_data.update(hindi_data)\n",
                "    print(f\"  BHSig Hindi: {len(hindi_data)} persons\")\n",
                "\n",
                "# Load BHSig Bengali  \n",
                "if os.path.exists(config.BHSIG_BENGALI):\n",
                "    bengali_data = load_dataset_images(config.BHSIG_BENGALI, 'bhsig')\n",
                "    all_data.update(bengali_data)\n",
                "    print(f\"  BHSig Bengali: {len(bengali_data)} persons\")\n",
                "\n",
                "print(f\"\\nTotal persons: {len(all_data)}\")\n",
                "\n",
                "# Generate pairs\n",
                "pairs, labels = generate_balanced_pairs(all_data, max_pairs_per_person=100)\n",
                "print(f\"Total balanced pairs: {len(pairs)}\")\n",
                "print(f\"Genuine pairs: {sum(labels)}\")\n",
                "print(f\"Forgery pairs: {len(labels) - sum(labels)}\")\n",
                "\n",
                "# Split\n",
                "split_idx = int(len(pairs) * config.TRAIN_SPLIT)\n",
                "train_pairs, train_labels = pairs[:split_idx], labels[:split_idx]\n",
                "val_pairs, val_labels = pairs[split_idx:], labels[split_idx:]\n",
                "\n",
                "print(f\"\\nTraining pairs: {len(train_pairs)}\")\n",
                "print(f\"Validation pairs: {len(val_pairs)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 7: Create DataLoaders\n",
                "\n",
                "# Transforms\n",
                "train_transform = transforms.Compose([\n",
                "    transforms.Resize(config.IMAGE_SIZE),\n",
                "    transforms.RandomRotation(5),\n",
                "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
                "])\n",
                "\n",
                "val_transform = transforms.Compose([\n",
                "    transforms.Resize(config.IMAGE_SIZE),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
                "])\n",
                "\n",
                "# Datasets\n",
                "train_dataset = SignatureDataset(train_pairs, train_labels, train_transform)\n",
                "val_dataset = SignatureDataset(val_pairs, val_labels, val_transform)\n",
                "\n",
                "# DataLoaders\n",
                "train_loader = DataLoader(\n",
                "    train_dataset, batch_size=config.BATCH_SIZE, shuffle=True,\n",
                "    num_workers=config.NUM_WORKERS, pin_memory=True\n",
                ")\n",
                "val_loader = DataLoader(\n",
                "    val_dataset, batch_size=config.BATCH_SIZE, shuffle=False,\n",
                "    num_workers=config.NUM_WORKERS, pin_memory=True\n",
                ")\n",
                "\n",
                "print(f\"‚úÖ DataLoaders ready!\")\n",
                "print(f\"   Train batches: {len(train_loader)}\")\n",
                "print(f\"   Val batches: {len(val_loader)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 8: Siamese Network Model\n",
                "\n",
                "class SiameseNetwork(nn.Module):\n",
                "    def __init__(self, embedding_dim=512):\n",
                "        super(SiameseNetwork, self).__init__()\n",
                "        \n",
                "        # MobileNetV2 backbone\n",
                "        self.backbone = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
                "        \n",
                "        # Modify for grayscale\n",
                "        if config.GRAYSCALE:\n",
                "            self.backbone.features[0][0] = nn.Conv2d(\n",
                "                1, 32, kernel_size=3, stride=2, padding=1, bias=False\n",
                "            )\n",
                "        \n",
                "        self.backbone.classifier = nn.Identity()\n",
                "        \n",
                "        # Embedding layers\n",
                "        self.embedding = nn.Sequential(\n",
                "            nn.Linear(1280, 1024),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Dropout(0.5),\n",
                "            nn.Linear(1024, embedding_dim),\n",
                "            nn.ReLU(inplace=True)\n",
                "        )\n",
                "        \n",
                "        # Classifier\n",
                "        self.classifier = nn.Sequential(\n",
                "            nn.Linear(embedding_dim, 256),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Dropout(0.3),\n",
                "            nn.Linear(256, 1),\n",
                "            nn.Sigmoid()\n",
                "        )\n",
                "    \n",
                "    def forward_one(self, x):\n",
                "        features = self.backbone(x)\n",
                "        if len(features.shape) > 2:\n",
                "            features = F.adaptive_avg_pool2d(features, 1)\n",
                "            features = features.view(features.size(0), -1)\n",
                "        return self.embedding(features)\n",
                "    \n",
                "    def forward(self, img1, img2):\n",
                "        emb1 = self.forward_one(img1)\n",
                "        emb2 = self.forward_one(img2)\n",
                "        diff = torch.abs(emb1 - emb2)\n",
                "        return self.classifier(diff).squeeze()\n",
                "\n",
                "# Create model\n",
                "model = SiameseNetwork(config.EMBEDDING_DIM).to(device)\n",
                "\n",
                "# Print summary\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "print(f\"‚úÖ Model created!\")\n",
                "print(f\"   Total params: {total_params:,}\")\n",
                "print(f\"   Trainable: {trainable:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 9: Training Functions\n",
                "\n",
                "def train_epoch(model, loader, criterion, optimizer):\n",
                "    model.train()\n",
                "    running_loss, correct, total = 0.0, 0, 0\n",
                "    \n",
                "    for img1, img2, labels in tqdm(loader, desc='Training'):\n",
                "        img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(img1, img2)\n",
                "        loss = criterion(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        running_loss += loss.item()\n",
                "        preds = (outputs > 0.5).float()\n",
                "        correct += (preds == labels).sum().item()\n",
                "        total += labels.size(0)\n",
                "    \n",
                "    return running_loss / len(loader), 100 * correct / total\n",
                "\n",
                "\n",
                "def validate(model, loader, criterion):\n",
                "    model.eval()\n",
                "    running_loss, correct, total = 0.0, 0, 0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for img1, img2, labels in tqdm(loader, desc='Validating'):\n",
                "            img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
                "            outputs = model(img1, img2)\n",
                "            loss = criterion(outputs, labels)\n",
                "            \n",
                "            running_loss += loss.item()\n",
                "            preds = (outputs > 0.5).float()\n",
                "            correct += (preds == labels).sum().item()\n",
                "            total += labels.size(0)\n",
                "    \n",
                "    return running_loss / len(loader), 100 * correct / total\n",
                "\n",
                "print(\"‚úÖ Training functions defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 10: Training Loop\n",
                "\n",
                "criterion = nn.BCELoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
                "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
                "\n",
                "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
                "best_val_acc = 0.0\n",
                "no_improve = 0\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"üöÄ Starting Training\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "start_time = time.time()\n",
                "\n",
                "for epoch in range(config.NUM_EPOCHS):\n",
                "    print(f\"\\nEpoch {epoch+1}/{config.NUM_EPOCHS}\")\n",
                "    \n",
                "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
                "    val_loss, val_acc = validate(model, val_loader, criterion)\n",
                "    \n",
                "    scheduler.step(val_acc)\n",
                "    \n",
                "    history['train_loss'].append(train_loss)\n",
                "    history['train_acc'].append(train_acc)\n",
                "    history['val_loss'].append(val_loss)\n",
                "    history['val_acc'].append(val_acc)\n",
                "    \n",
                "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
                "    print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
                "    \n",
                "    # Save best model\n",
                "    if val_acc > best_val_acc:\n",
                "        best_val_acc = val_acc\n",
                "        no_improve = 0\n",
                "        torch.save({\n",
                "            'epoch': epoch,\n",
                "            'model_state_dict': model.state_dict(),\n",
                "            'val_acc': val_acc\n",
                "        }, os.path.join(config.SAVE_PATH, 'best_model.pth'))\n",
                "        print(f\"  ‚≠ê Best model saved! Acc: {val_acc:.2f}%\")\n",
                "    else:\n",
                "        no_improve += 1\n",
                "        print(f\"  ‚ö†Ô∏è No improvement ({no_improve}/{config.EARLY_STOPPING_PATIENCE})\")\n",
                "    \n",
                "    # Periodic checkpoint\n",
                "    if (epoch + 1) % config.CHECKPOINT_INTERVAL == 0:\n",
                "        torch.save({\n",
                "            'epoch': epoch,\n",
                "            'model_state_dict': model.state_dict(),\n",
                "            'val_acc': val_acc,\n",
                "            'history': history\n",
                "        }, os.path.join(config.SAVE_PATH, f'checkpoint_epoch_{epoch+1}.pth'))\n",
                "        print(f\"  üìÅ Checkpoint saved: epoch {epoch+1}\")\n",
                "    \n",
                "    # Early stopping\n",
                "    if no_improve >= config.EARLY_STOPPING_PATIENCE:\n",
                "        print(f\"\\n‚õî Early stopping at epoch {epoch+1}\")\n",
                "        break\n",
                "\n",
                "total_time = time.time() - start_time\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"üéâ Training Complete!\")\n",
                "print(f\"   Time: {total_time/60:.1f} minutes\")\n",
                "print(f\"   Best Val Acc: {best_val_acc:.2f}%\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 11: Plot Training History\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Loss\n",
                "axes[0].plot(history['train_loss'], label='Train', linewidth=2)\n",
                "axes[0].plot(history['val_loss'], label='Validation', linewidth=2)\n",
                "axes[0].set_xlabel('Epoch')\n",
                "axes[0].set_ylabel('Loss')\n",
                "axes[0].set_title('Training and Validation Loss')\n",
                "axes[0].legend()\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "\n",
                "# Accuracy\n",
                "axes[1].plot(history['train_acc'], label='Train', linewidth=2)\n",
                "axes[1].plot(history['val_acc'], label='Validation', linewidth=2)\n",
                "axes[1].set_xlabel('Epoch')\n",
                "axes[1].set_ylabel('Accuracy (%)')\n",
                "axes[1].set_title('Training and Validation Accuracy')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(config.SAVE_PATH, 'training_history.png'), dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 12: List Saved Models\n",
                "\n",
                "print(\"üìÅ Saved Models:\")\n",
                "print(\"=\"*50)\n",
                "for f in sorted(os.listdir(config.SAVE_PATH)):\n",
                "    if f.endswith('.pth'):\n",
                "        path = os.path.join(config.SAVE_PATH, f)\n",
                "        size = os.path.getsize(path) / (1024*1024)\n",
                "        print(f\"  {f} ({size:.1f} MB)\")\n",
                "\n",
                "print(\"\\nüí° Download the best_model.pth file for deployment!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}